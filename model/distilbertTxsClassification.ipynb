{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "7HSra46IFCIw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Caio\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import transformers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAByusTUE-sH",
        "outputId": "3ac2d599-3b46-4047-8821-44bb78406333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de rótulos únicos: 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassificationWithDropout were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "c:\\Users\\Caio\\Documents\\TCC\\myenv\\lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Average Loss: 2.205546554766203\n",
            "Epoch 2 - Average Loss: 1.757932041820727\n",
            "Epoch 3 - Average Loss: 1.1837029849228107\n",
            "Epoch 4 - Average Loss: 0.7815493141349993\n",
            "Epoch 5 - Average Loss: 0.541065196457662\n",
            "Epoch 6 - Average Loss: 0.41603478867756694\n",
            "Epoch 7 - Average Loss: 0.34453421125286504\n",
            "Epoch 8 - Average Loss: 0.29953567683696747\n",
            "Epoch 9 - Average Loss: 0.2794254398659656\n",
            "Epoch 10 - Average Loss: 0.2694174529690492\n",
            "Pesos do modelo treinado salvos com sucesso\n",
            "Empty DataFrame\n",
            "Columns: [Text, Real Label, Predicted Label]\n",
            "Index: []\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "  Entertainment       1.00      1.00      1.00        30\n",
            "            Gas       1.00      1.00      1.00        28\n",
            "      Groceries       1.00      1.00      1.00        35\n",
            "Interest Income       1.00      1.00      1.00        29\n",
            "       Pharmacy       1.00      1.00      1.00        30\n",
            "    Restaurants       1.00      1.00      1.00        27\n",
            "         Salary       1.00      1.00      1.00        34\n",
            "       Shopping       1.00      1.00      1.00        25\n",
            "   Transfer-Out       1.00      1.00      1.00        31\n",
            "      Utilities       1.00      1.00      1.00        31\n",
            "\n",
            "       accuracy                           1.00       300\n",
            "      macro avg       1.00      1.00      1.00       300\n",
            "   weighted avg       1.00      1.00      1.00       300\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Criar a função de pré-processamento do texto\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Converter para minúsculas\n",
        "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)  # Remover palavras com 1 ou 2 caracteres\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remover espaços extras\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remover stop words\n",
        "    return text\n",
        "\n",
        "# Carregar o dataset\n",
        "data = pd.read_csv('extended_transactions_large_unique.csv')  # caminho do arquivo\n",
        "\n",
        "data_sample = data.sample(1500, random_state=42)\n",
        "\n",
        "# Extrair as descrições das transações e etiquetas\n",
        "texts = data_sample['x'].apply(preprocess_text).tolist()\n",
        "labels = data_sample['y'].tolist()\n",
        "\n",
        "# Mapear as etiquetas para valores numéricos\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Verificar o número de rótulos únicos\n",
        "num_labels = len(set(labels))\n",
        "print(\"Número de rótulos únicos:\", num_labels)\n",
        "assert num_labels == 10, \"O número de rótulos únicos deve ser 10.\"\n",
        "\n",
        "# Dividir os dados em conjuntos de treinamento e teste\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Carregar o tokenizer do DistilBERT\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Tokenizar os textos e convertê-los para tensores de entrada\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "# Converter as etiquetas para tensores\n",
        "train_labels = torch.tensor(train_labels)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "# Criar o modelo DistilBERT para classificação de sequências com Dropout\n",
        "class DistilBertForSequenceClassificationWithDropout(DistilBertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.dropout = torch.nn.Dropout(p=0.5)  # Adicionando dropout de 50%\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
        "        outputs = self.distilbert(input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs[0][:, 0]  # Usar a saída [CLS]\n",
        "        pooled_output = self.pre_classifier(pooled_output)\n",
        "        pooled_output = torch.nn.ReLU()(pooled_output)\n",
        "        pooled_output = self.dropout(pooled_output)  # Aplicando dropout\n",
        "        logits = self.classifier(pooled_output)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = torch.nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "        output = (loss, logits) if loss is not None else (logits,)\n",
        "        return output\n",
        "\n",
        "model = DistilBertForSequenceClassificationWithDropout.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
        "\n",
        "# Ponderar as classes para lidar com desequilíbrios\n",
        "class_counts = np.bincount(train_labels)\n",
        "class_weights = 1. / class_counts\n",
        "weights = class_weights[train_labels]\n",
        "sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "\n",
        "# Criar um DataLoader para os dados de treinamento usando o sampler ponderado\n",
        "train_dataset = torch.utils.data.TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, sampler=sampler)  # Ajustar o tamanho do lote\n",
        "\n",
        "# Configurar o otimizador e o scheduler de taxa de aprendizado com uma taxa de aprendizado reduzida\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)  # Reduzir a taxa de aprendizado\n",
        "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * 10)  # numero de epocas como 10\n",
        "\n",
        "# Loop de treinamento\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(10):  # Aumentar o número de épocas para 10\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids, attention_mask, batch_labels = batch\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
        "        loss = outputs[0]  # Extrair a perda do primeiro item da tupla\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    \n",
        "    print(f\"Epoch {epoch+1} - Average Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "# Salvar os pesos do modelo treinado\n",
        "torch.save(model.state_dict(), 'model_weights.pth')\n",
        "\n",
        "# Verificar se os pesos foram salvos corretamente\n",
        "print(\"Pesos do modelo treinado salvos com sucesso\")\n",
        "\n",
        "# Avaliação\n",
        "model.eval()\n",
        "test_encodings = {key: val.to(device) for key, val in test_encodings.items()}\n",
        "with torch.no_grad():\n",
        "    outputs = model(**test_encodings)\n",
        "    logits = outputs[0]  # Extrair logits do primeiro item da tupla\n",
        "    predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "# Converter as etiquetas previstas e as etiquetas reais para CPU\n",
        "predictions = predictions.cpu().numpy()\n",
        "test_labels = test_labels.cpu().numpy()\n",
        "\n",
        "\n",
        "# Criar um DataFrame com as previsões e as etiquetas reais\n",
        "df_predictions = pd.DataFrame({\n",
        "    'Text': test_texts,\n",
        "    'Real Label': label_encoder.inverse_transform(test_labels),\n",
        "    'Predicted Label': label_encoder.inverse_transform(predictions)\n",
        "})\n",
        "\n",
        "# Filtrar as previsões incorretas\n",
        "df_incorrect_predictions = df_predictions[df_predictions['Real Label'] != df_predictions['Predicted Label']]\n",
        "\n",
        "# Exibir as previsões incorretas\n",
        "print(df_incorrect_predictions)\n",
        "\n",
        "\n",
        "# Relatório de Classificação Detalhado\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
        "    report = classification_report(test_labels, predictions, target_names=label_encoder.classes_, zero_division=1)\n",
        "\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
